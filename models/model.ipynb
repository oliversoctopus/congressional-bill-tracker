{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f40241-19a7-4449-b530-275ee7129d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TIME-AWARE MODEL TRAINING - 6 CONGRESS DATASET\n",
      "============================================================\n",
      "Loading dataset from: ../data/bills_6congress_training.csv\n",
      "Loaded 76897 bills with known outcomes\n",
      "Congresses: [np.int64(113), np.int64(114), np.int64(115), np.int64(116), np.int64(117), np.int64(118)]\n",
      "Passed bills: 2056 (2.7%)\n",
      "\n",
      "============================================================\n",
      "CREATING VIABILITY TARGET\n",
      "============================================================\n",
      "Viable bills: 12315 (16.0%)\n",
      "\n",
      "Creating enhanced features...\n",
      "\n",
      "Verifying feature availability...\n",
      "\n",
      "Feature counts:\n",
      "- Base: 17\n",
      "- Extended: 25\n",
      "- Progressive: 39\n",
      "\n",
      "======================================================================\n",
      "PHASE 1: VIABILITY PREDICTION\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "TRAINING VIABILITY MODELS\n",
      "============================================================\n",
      "\n",
      "--- Training new_bill model ---\n",
      "Using 17 features\n",
      "Positive class rate: 16.0%\n",
      "Selected top 16 features\n",
      "Training individual models...\n",
      "Training ensemble model...\n",
      "Calibrating probabilities...\n",
      "\n",
      "Performance (threshold=0.500):\n",
      "  Accuracy: 0.8875\n",
      "  ROC-AUC: 0.8856\n",
      "  Precision: 0.7253\n",
      "  Recall: 0.4791\n",
      "  F1 Score: 0.5770\n",
      "  CV ROC-AUC: 0.8883 (+/- 0.0083)\n",
      "\n",
      "Top 5 features:\n",
      "  - subject_count: 0.1601\n",
      "  - has_bipartisan_support: 0.1328\n",
      "  - bipartisan_score: 0.1018\n",
      "  - party_dominance: 0.0946\n",
      "  - congress_numeric: 0.0822\n",
      "\n",
      "--- Training early_stage model ---\n",
      "Using 25 features\n",
      "Positive class rate: 16.0%\n",
      "Selected top 20 features\n",
      "Training individual models...\n",
      "Training ensemble model...\n",
      "Calibrating probabilities...\n",
      "\n",
      "Performance (threshold=0.500):\n",
      "  Accuracy: 0.9850\n",
      "  ROC-AUC: 0.9942\n",
      "  Precision: 0.9869\n",
      "  Recall: 0.9188\n",
      "  F1 Score: 0.9516\n",
      "  CV ROC-AUC: 0.9944 (+/- 0.0017)\n",
      "\n",
      "Top 5 features:\n",
      "  - early_activity: 0.3121\n",
      "  - is_fresh: 0.1809\n",
      "  - support_velocity: 0.1037\n",
      "  - total_sponsors: 0.0970\n",
      "  - cosponsor_count: 0.0890\n",
      "\n",
      "--- Training progressive model ---\n",
      "Using 39 features\n",
      "Positive class rate: 16.0%\n",
      "Selected top 20 features\n",
      "Training individual models...\n",
      "Training ensemble model...\n",
      "Calibrating probabilities...\n",
      "\n",
      "Performance (threshold=0.500):\n",
      "  Accuracy: 0.9910\n",
      "  ROC-AUC: 0.9956\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.9440\n",
      "  F1 Score: 0.9712\n",
      "  CV ROC-AUC: 0.9959 (+/- 0.0011)\n",
      "\n",
      "Top 5 features:\n",
      "  - action_count: 0.2138\n",
      "  - days_active: 0.1086\n",
      "  - total_sponsors: 0.0856\n",
      "  - log_days_active: 0.0825\n",
      "  - cosponsor_count: 0.0819\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: PASSAGE PREDICTION\n",
      "Training on 12315 viable bills (16.7% passed)\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "TRAINING PASSAGE MODELS\n",
      "============================================================\n",
      "\n",
      "--- Training new_bill model ---\n",
      "Using 17 features\n",
      "Positive class rate: 16.7%\n",
      "Selected top 16 features\n",
      "Training individual models...\n",
      "Training ensemble model...\n",
      "Calibrating probabilities...\n",
      "\n",
      "Performance (threshold=0.500):\n",
      "  Accuracy: 0.8603\n",
      "  ROC-AUC: 0.8122\n",
      "  Precision: 0.7724\n",
      "  Recall: 0.2311\n",
      "  F1 Score: 0.3558\n",
      "  CV ROC-AUC: 0.8024 (+/- 0.0221)\n",
      "\n",
      "Top 5 features:\n",
      "  - bipartisan_score: 0.1589\n",
      "  - party_dominance: 0.1551\n",
      "  - policy_area_encoded: 0.1135\n",
      "  - party_balance: 0.1105\n",
      "  - title_length: 0.0928\n",
      "\n",
      "--- Training early_stage model ---\n",
      "Using 25 features\n",
      "Positive class rate: 16.7%\n",
      "Selected top 20 features\n",
      "Training individual models...\n",
      "Training ensemble model...\n",
      "Calibrating probabilities...\n",
      "\n",
      "Performance (threshold=0.500):\n",
      "  Accuracy: 0.9395\n",
      "  ROC-AUC: 0.9761\n",
      "  Precision: 0.8149\n",
      "  Recall: 0.8248\n",
      "  F1 Score: 0.8198\n",
      "  CV ROC-AUC: 0.9763 (+/- 0.0098)\n",
      "\n",
      "Top 5 features:\n",
      "  - early_activity: 0.5106\n",
      "  - support_velocity: 0.0820\n",
      "  - cosponsor_growth: 0.0708\n",
      "  - is_fresh: 0.0527\n",
      "  - bipartisan_score: 0.0468\n",
      "\n",
      "--- Training progressive model ---\n",
      "Using 39 features\n",
      "Positive class rate: 16.7%\n",
      "Selected top 20 features\n",
      "Training individual models...\n",
      "Training ensemble model...\n",
      "Calibrating probabilities...\n",
      "\n",
      "Performance (threshold=0.500):\n",
      "  Accuracy: 0.9395\n",
      "  ROC-AUC: 0.9786\n",
      "  Precision: 0.8061\n",
      "  Recall: 0.8394\n",
      "  F1 Score: 0.8224\n",
      "  CV ROC-AUC: 0.9803 (+/- 0.0053)\n",
      "\n",
      "Top 5 features:\n",
      "  - action_count: 0.3181\n",
      "  - early_activity: 0.1593\n",
      "  - normalized_activity: 0.1245\n",
      "  - sustained_activity: 0.0701\n",
      "  - bipartisan_momentum: 0.0592\n",
      "\n",
      "============================================================\n",
      "MODEL EVALUATION BY CONGRESS\n",
      "============================================================\n",
      "\n",
      "113th Congress (n=9091):\n",
      "  Viability rate: 18.9%\n",
      "  Passage rate: 3.2%\n",
      "\n",
      "114th Congress (n=10233):\n",
      "  Viability rate: 19.2%\n",
      "  Passage rate: 3.3%\n",
      "\n",
      "115th Congress (n=11421):\n",
      "  Viability rate: 19.6%\n",
      "  Passage rate: 3.8%\n",
      "\n",
      "116th Congress (n=14345):\n",
      "  Viability rate: 15.0%\n",
      "  Passage rate: 2.4%\n",
      "\n",
      "117th Congress (n=15242):\n",
      "  Viability rate: 14.4%\n",
      "  Passage rate: 2.4%\n",
      "\n",
      "118th Congress (n=16565):\n",
      "  Viability rate: 12.4%\n",
      "  Passage rate: 1.7%\n",
      "\n",
      "============================================================\n",
      "SAVING MODELS IN OPTIMIZED SPLIT COMPONENTS\n",
      "============================================================\n",
      "✅ Saved metadata.pkl\n",
      "\n",
      "--- Saving viability_new_bill components ---\n",
      "  ✅ Saved rf_model.pkl (56.4 MB)\n",
      "  ✅ Saved components.pkl (0.8 MB) - contains gb_model, lr_model, scaler, selector, metadata\n",
      "  ✅ Saved ensemble_config.pkl (<0.1 MB)\n",
      "  ✅ Saved calibration.pkl (0.0 MB)\n",
      "  Total size for viability_new_bill: 57.3 MB\n",
      "\n",
      "--- Saving viability_early_stage components ---\n",
      "  ✅ Saved rf_model.pkl (28.2 MB)\n",
      "  ✅ Saved components.pkl (0.8 MB) - contains gb_model, lr_model, scaler, selector, metadata\n",
      "  ✅ Saved ensemble_config.pkl (<0.1 MB)\n",
      "  ✅ Saved calibration.pkl (0.0 MB)\n",
      "  Total size for viability_early_stage: 29.2 MB\n",
      "\n",
      "--- Saving viability_progressive components ---\n",
      "  ✅ Saved rf_model.pkl (13.4 MB)\n",
      "  ✅ Saved components.pkl (0.6 MB) - contains gb_model, lr_model, scaler, selector, metadata\n",
      "  ✅ Saved ensemble_config.pkl (<0.1 MB)\n",
      "  ✅ Saved calibration.pkl (0.0 MB)\n",
      "  Total size for viability_progressive: 14.2 MB\n",
      "\n",
      "--- Saving passage_new_bill components ---\n",
      "  ✅ Saved rf_model.pkl (14.7 MB)\n",
      "  ✅ Saved components.pkl (0.8 MB) - contains gb_model, lr_model, scaler, selector, metadata\n",
      "  ✅ Saved ensemble_config.pkl (<0.1 MB)\n",
      "  ✅ Saved calibration.pkl (0.0 MB)\n",
      "  Total size for passage_new_bill: 15.6 MB\n",
      "\n",
      "--- Saving passage_early_stage components ---\n",
      "  ✅ Saved rf_model.pkl (8.3 MB)\n",
      "  ✅ Saved components.pkl (0.7 MB) - contains gb_model, lr_model, scaler, selector, metadata\n",
      "  ✅ Saved ensemble_config.pkl (<0.1 MB)\n",
      "  ✅ Saved calibration.pkl (0.0 MB)\n",
      "  Total size for passage_early_stage: 9.1 MB\n",
      "\n",
      "--- Saving passage_progressive components ---\n",
      "  ✅ Saved rf_model.pkl (6.0 MB)\n",
      "  ✅ Saved components.pkl (0.6 MB) - contains gb_model, lr_model, scaler, selector, metadata\n",
      "  ✅ Saved ensemble_config.pkl (<0.1 MB)\n",
      "  ✅ Saved calibration.pkl (0.0 MB)\n",
      "  Total size for passage_progressive: 6.7 MB\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING COMPLETE!\n",
      "Models saved in 'models' directory as optimized components\n",
      "Large RF models saved separately, small components combined\n",
      "Trained on 76897 bills from congresses: [np.int64(113), np.int64(114), np.int64(115), np.int64(116), np.int64(117), np.int64(118)]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TIME-AWARE DUAL MODEL APPROACH FOR 6-CONGRESS DATASET - OPTIMIZED SPLIT COMPONENTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TIME-AWARE MODEL TRAINING - 6 CONGRESS DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load data - use the training dataset with known outcomes\n",
    "dataset_path = '../data/bills_6congress_training.csv'\n",
    "print(f\"Loading dataset from: {dataset_path}\")\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(f\"Loaded {len(df)} bills with known outcomes\")\n",
    "print(f\"Congresses: {sorted(df['congress'].unique())}\")\n",
    "print(f\"Passed bills: {df['passed'].sum()} ({df['passed'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Viability definition\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING VIABILITY TARGET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Balanced viability criteria - aim for ~15-20% viable\n",
    "df['viable'] = (\n",
    "    (df['passed'] == 1) |  # Passed\n",
    "    ((df.get('action_count', 0) >= 6) & (df.get('committee_count', 0) >= 1)) |  # Good activity + committee\n",
    "    ((df.get('cosponsor_count', 0) >= 30) & (df.get('action_count', 0) >= 4)) |  # Strong support + some activity\n",
    "    ((df.get('action_count', 0) >= 10)) |  # Very high activity alone\n",
    "    (df.get('failure_reason', '') == 'failed_to_complete')  # Almost passed\n",
    ").astype(int)\n",
    "\n",
    "# Add milestone-based viability\n",
    "if 'latest_action' in df.columns:\n",
    "    strong_milestones = ['passed', 'reported', 'ordered reported', 'markup', 'hearing held']\n",
    "    pattern = '|'.join(strong_milestones)\n",
    "    df['has_strong_milestone'] = df['latest_action'].fillna('').str.lower().str.contains(pattern)\n",
    "    df['viable'] = df['viable'] | df['has_strong_milestone']\n",
    "    df['viable'] = df['viable'].astype(int)\n",
    "\n",
    "print(f\"Viable bills: {df['viable'].sum()} ({df['viable'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Feature engineering adapted for 6-congress dataset\n",
    "print(\"\\nCreating enhanced features...\")\n",
    "\n",
    "# Handle column existence more carefully\n",
    "def safe_get_column(df, col_name, default_value):\n",
    "    \"\"\"Safely get a column with a default value if it doesn't exist\"\"\"\n",
    "    if col_name in df.columns:\n",
    "        return df[col_name].fillna(default_value)\n",
    "    else:\n",
    "        return default_value\n",
    "\n",
    "# Fill missing values with sensible defaults\n",
    "df['sponsor_party'] = safe_get_column(df, 'sponsor_party', 'Unknown')\n",
    "df['policy_area'] = safe_get_column(df, 'policy_area', 'Unknown')\n",
    "\n",
    "# Use days_active instead of days_since_introduction (from our preprocessing)\n",
    "if 'days_active' not in df.columns:\n",
    "    # If somehow missing, calculate from dates\n",
    "    if 'latest_action_date' in df.columns and 'introduced_date' in df.columns:\n",
    "        df['introduced_date'] = pd.to_datetime(df['introduced_date'])\n",
    "        df['latest_action_date'] = pd.to_datetime(df['latest_action_date'])\n",
    "        df['days_active'] = (df['latest_action_date'] - df['introduced_date']).dt.days.fillna(30).clip(1, 730)\n",
    "    else:\n",
    "        df['days_active'] = 30\n",
    "\n",
    "df['action_count'] = safe_get_column(df, 'action_count', 1).clip(1, 100)\n",
    "df['committee_count'] = safe_get_column(df, 'committee_count', 0)\n",
    "df['cosponsor_count'] = safe_get_column(df, 'cosponsor_count', 0)\n",
    "df['original_cosponsor_count'] = safe_get_column(df, 'original_cosponsor_count', 0)\n",
    "\n",
    "# Label encoding - handle unknown categories\n",
    "le_party = LabelEncoder()\n",
    "# Fit on all possible values including 'Unknown'\n",
    "all_parties = list(df['sponsor_party'].unique()) + ['Unknown']\n",
    "le_party.fit(all_parties)\n",
    "df['sponsor_party_encoded'] = le_party.transform(df['sponsor_party'])\n",
    "\n",
    "le_policy = LabelEncoder()\n",
    "all_policies = list(df['policy_area'].unique()) + ['Unknown']\n",
    "le_policy.fit(all_policies)\n",
    "df['policy_area_encoded'] = le_policy.transform(df['policy_area'])\n",
    "\n",
    "# Enhanced feature engineering\n",
    "# Basic sponsor features\n",
    "df['dem_sponsors'] = safe_get_column(df, 'dem_sponsors', 0)\n",
    "df['rep_sponsors'] = safe_get_column(df, 'rep_sponsors', 0)\n",
    "df['ind_sponsors'] = safe_get_column(df, 'ind_sponsors', 0)\n",
    "df['dem_cosponsors'] = safe_get_column(df, 'dem_cosponsors', 0)\n",
    "df['rep_cosponsors'] = safe_get_column(df, 'rep_cosponsors', 0)\n",
    "\n",
    "df['sponsor_count'] = (df['dem_sponsors'] + df['rep_sponsors'] + df['ind_sponsors']).clip(1, 10)\n",
    "df['total_sponsors'] = df['sponsor_count'] + df['cosponsor_count']\n",
    "\n",
    "# Party balance features\n",
    "df['dem_total'] = df['dem_sponsors'] + df['dem_cosponsors']\n",
    "df['rep_total'] = df['rep_sponsors'] + df['rep_cosponsors']\n",
    "df['party_balance'] = (df['dem_total'] - df['rep_total']) / (df['total_sponsors'] + 1)\n",
    "df['party_dominance'] = abs(df['party_balance'])\n",
    "\n",
    "# Bipartisan features\n",
    "df['bipartisan_score'] = 1 - df['party_dominance']\n",
    "df['has_bipartisan_support'] = safe_get_column(df, 'is_bipartisan', 0)\n",
    "if 'has_bipartisan_support' not in df.columns or df['has_bipartisan_support'].isna().all():\n",
    "    df['has_bipartisan_support'] = ((df['dem_total'] > 0) & (df['rep_total'] > 0)).astype(int)\n",
    "\n",
    "# Temporal features\n",
    "df['month_introduced'] = safe_get_column(df, 'month_introduced', datetime.now().month)\n",
    "df['quarter_introduced'] = safe_get_column(df, 'quarter_introduced', ((datetime.now().month - 1) // 3 + 1))\n",
    "df['is_election_year'] = safe_get_column(df, 'is_election_year', 0)\n",
    "\n",
    "# Congress-specific features\n",
    "df['congress_numeric'] = df['congress'].astype(int)\n",
    "df['is_recent_congress'] = (df['congress_numeric'] >= 117).astype(int)\n",
    "\n",
    "# Text features\n",
    "df['title_length'] = safe_get_column(df, 'title_length', 100).clip(10, 500)\n",
    "df['title_word_count'] = safe_get_column(df, 'title_word_count', 20).clip(2, 100)\n",
    "df['title_complexity'] = df['title_length'] / (df['title_word_count'] + 1)\n",
    "\n",
    "# Subject features\n",
    "df['subject_count'] = safe_get_column(df, 'subject_count', 1).clip(1, 20)\n",
    "\n",
    "# Time-aware features with better scaling\n",
    "df['log_days_active'] = np.log1p(df['days_active'])\n",
    "df['sqrt_days_active'] = np.sqrt(df['days_active'])\n",
    "\n",
    "# Activity features - use existing if available\n",
    "if 'legislative_velocity' in df.columns:\n",
    "    df['activity_rate'] = df['legislative_velocity']\n",
    "else:\n",
    "    df['activity_rate'] = df['action_count'] / df['days_active']\n",
    "\n",
    "df['normalized_activity'] = df['action_count'] / np.log1p(df['days_active'])\n",
    "\n",
    "# Use existing temporal features if available\n",
    "if 'early_activity' not in df.columns:\n",
    "    df['early_activity'] = df['action_count'] / (df['days_active'].clip(upper=30) + 1)\n",
    "if 'sustained_activity' not in df.columns:\n",
    "    df['sustained_activity'] = df['action_count'] / (df['days_active'].clip(upper=180) + 1)\n",
    "\n",
    "# Momentum features\n",
    "df['is_fresh'] = (df['days_active'] <= 30).astype(int)\n",
    "df['is_active'] = (df['days_active'] <= 90).astype(int)\n",
    "df['is_stale'] = (df['days_active'] > 180).astype(int)\n",
    "\n",
    "# Committee features\n",
    "if 'committee_engagement_speed' not in df.columns:\n",
    "    df['committee_density'] = df['committee_count'] / (df['days_active'] / 30).clip(lower=1)\n",
    "else:\n",
    "    df['committee_density'] = df['committee_engagement_speed']\n",
    "\n",
    "df['has_committee'] = (df['committee_count'] > 0).astype(int)\n",
    "df['multi_committee'] = (df['committee_count'] >= 2).astype(int)\n",
    "\n",
    "# Support growth features\n",
    "df['cosponsor_growth'] = (df['cosponsor_count'] - df['original_cosponsor_count']) / (df['days_active'] / 30).clip(lower=1)\n",
    "df['support_velocity'] = df['total_sponsors'] / np.sqrt(df['days_active'])\n",
    "\n",
    "# Interaction features\n",
    "df['bipartisan_momentum'] = df['bipartisan_score'] * df['normalized_activity']\n",
    "df['committee_activity'] = df['committee_count'] * df['activity_rate']\n",
    "\n",
    "# Define feature sets\n",
    "base_features = [\n",
    "    'sponsor_party_encoded',\n",
    "    'sponsor_count',\n",
    "    'original_cosponsor_count',\n",
    "    'month_introduced',\n",
    "    'quarter_introduced',\n",
    "    'is_election_year',\n",
    "    'title_length',\n",
    "    'title_word_count',\n",
    "    'title_complexity',\n",
    "    'subject_count',\n",
    "    'policy_area_encoded',\n",
    "    'party_balance',\n",
    "    'party_dominance',\n",
    "    'bipartisan_score',\n",
    "    'has_bipartisan_support',\n",
    "    'congress_numeric',  # Added congress info\n",
    "    'is_recent_congress'  # Added to distinguish newer congresses\n",
    "]\n",
    "\n",
    "extended_features = base_features + [\n",
    "    'cosponsor_count',\n",
    "    'total_sponsors',\n",
    "    'is_fresh',\n",
    "    'support_velocity',\n",
    "    'cosponsor_growth',\n",
    "    'dem_total',\n",
    "    'rep_total',\n",
    "    'early_activity'  # Added from preprocessing\n",
    "]\n",
    "\n",
    "progressive_features = extended_features + [\n",
    "    'days_active',\n",
    "    'log_days_active',\n",
    "    'activity_rate',\n",
    "    'normalized_activity',\n",
    "    'sustained_activity',  # Added from preprocessing\n",
    "    'is_active',\n",
    "    'is_stale',\n",
    "    'committee_count',\n",
    "    'has_committee',\n",
    "    'multi_committee',\n",
    "    'committee_density',\n",
    "    'action_count',\n",
    "    'bipartisan_momentum',\n",
    "    'committee_activity'\n",
    "]\n",
    "\n",
    "# Filter out features that don't exist\n",
    "print(\"\\nVerifying feature availability...\")\n",
    "for feature_set_name, features in [('base', base_features), ('extended', extended_features), ('progressive', progressive_features)]:\n",
    "    missing = [f for f in features if f not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"Warning: Missing features in {feature_set_name}: {missing}\")\n",
    "\n",
    "# Remove missing features from sets\n",
    "base_features = [f for f in base_features if f in df.columns]\n",
    "extended_features = [f for f in extended_features if f in df.columns]\n",
    "progressive_features = [f for f in progressive_features if f in df.columns]\n",
    "\n",
    "print(f\"\\nFeature counts:\")\n",
    "print(f\"- Base: {len(base_features)}\")\n",
    "print(f\"- Extended: {len(extended_features)}\")\n",
    "print(f\"- Progressive: {len(progressive_features)}\")\n",
    "\n",
    "# Model training function remains the same\n",
    "def train_robust_model(df, target_col, model_name, feature_sets, use_calibration=True):\n",
    "    \"\"\"Train models with better methodology\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING {model_name.upper()} MODELS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    for stage_name, features in feature_sets.items():\n",
    "        print(f\"\\n--- Training {stage_name} model ---\")\n",
    "        \n",
    "        # Filter available features\n",
    "        available_features = [f for f in features if f in df.columns]\n",
    "        print(f\"Using {len(available_features)} features\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X = df[available_features].fillna(0)\n",
    "        X = X.replace([np.inf, -np.inf], 0)\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Check class distribution\n",
    "        pos_rate = y.mean()\n",
    "        print(f\"Positive class rate: {pos_rate:.1%}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X_train),\n",
    "            columns=X_train.columns,\n",
    "            index=X_train.index\n",
    "        )\n",
    "        X_test_scaled = pd.DataFrame(\n",
    "            scaler.transform(X_test),\n",
    "            columns=X_test.columns,\n",
    "            index=X_test.index\n",
    "        )\n",
    "        \n",
    "        # Feature selection\n",
    "        k_features = min(20, len(available_features) - 1)\n",
    "        selector = SelectKBest(mutual_info_classif, k=k_features)\n",
    "        selector.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        selected_features = [available_features[i] for i in selected_indices]\n",
    "        \n",
    "        X_train_selected = X_train_scaled[selected_features]\n",
    "        X_test_selected = X_test_scaled[selected_features]\n",
    "        \n",
    "        print(f\"Selected top {k_features} features\")\n",
    "        \n",
    "        # Create diverse ensemble\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=15,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            class_weight='balanced_subsample',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        gb_model = GradientBoostingClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        lr_model = LogisticRegression(\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train individual models\n",
    "        print(\"Training individual models...\")\n",
    "        rf_model.fit(X_train_selected, y_train)\n",
    "        gb_model.fit(X_train_selected, y_train)\n",
    "        lr_model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        # Create voting ensemble\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_model),\n",
    "                ('gb', gb_model),\n",
    "                ('lr', lr_model)\n",
    "            ],\n",
    "            voting='soft',\n",
    "            weights=[0.4, 0.4, 0.2]\n",
    "        )\n",
    "        \n",
    "        # Train ensemble\n",
    "        print(\"Training ensemble model...\")\n",
    "        ensemble.fit(X_train_selected, y_train)\n",
    "        \n",
    "        # Calibrate if needed\n",
    "        if use_calibration and pos_rate < 0.3:\n",
    "            print(\"Calibrating probabilities...\")\n",
    "            calibrated_ensemble = CalibratedClassifierCV(\n",
    "                ensemble, \n",
    "                method='isotonic',\n",
    "                cv=3\n",
    "            )\n",
    "            calibrated_ensemble.fit(X_train_selected, y_train)\n",
    "            final_model = calibrated_ensemble\n",
    "        else:\n",
    "            final_model = ensemble\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_proba = final_model.predict_proba(X_test_selected)[:, 1]\n",
    "        \n",
    "        # Use adaptive threshold\n",
    "        if pos_rate < 0.15:\n",
    "            threshold = pos_rate * 2\n",
    "        else:\n",
    "            threshold = 0.5\n",
    "        \n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        print(f\"\\nPerformance (threshold={threshold:.3f}):\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(\n",
    "            ensemble, X_train_selected, y_train, \n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            scoring='roc_auc'\n",
    "        )\n",
    "        print(f\"  CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "        \n",
    "        # Store model components\n",
    "        models[stage_name] = {\n",
    "            'model': final_model,\n",
    "            'ensemble': ensemble,\n",
    "            'rf_model': rf_model,\n",
    "            'gb_model': gb_model,\n",
    "            'lr_model': lr_model,\n",
    "            'scaler': scaler,\n",
    "            'selector': selector,\n",
    "            'features': available_features,\n",
    "            'selected_features': selected_features,\n",
    "            'threshold': threshold,\n",
    "            'performance': {\n",
    "                'accuracy': accuracy,\n",
    "                'roc_auc': roc_auc,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'cv_roc_auc': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std()\n",
    "            },\n",
    "            'is_calibrated': use_calibration and pos_rate < 0.3\n",
    "        }\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(rf_model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': selected_features,\n",
    "                'importance': rf_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop 5 features:\")\n",
    "            for _, row in importance_df.head(5).iterrows():\n",
    "                print(f\"  - {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Define feature sets\n",
    "feature_sets = {\n",
    "    'new_bill': base_features,\n",
    "    'early_stage': extended_features,\n",
    "    'progressive': progressive_features\n",
    "}\n",
    "\n",
    "# Train viability models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1: VIABILITY PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "viability_models = train_robust_model(df, 'viable', 'Viability', feature_sets)\n",
    "\n",
    "# Train passage models on viable bills only\n",
    "viable_bills = df[df['viable'] == 1].copy()\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"PHASE 2: PASSAGE PREDICTION\")\n",
    "print(f\"Training on {len(viable_bills)} viable bills ({viable_bills['passed'].mean():.1%} passed)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "passage_models = train_robust_model(viable_bills, 'passed', 'Passage', feature_sets)\n",
    "\n",
    "# Model evaluation by congress\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION BY CONGRESS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for congress in sorted(df['congress'].unique()):\n",
    "    congress_df = df[df['congress'] == congress]\n",
    "    print(f\"\\n{congress}th Congress (n={len(congress_df)}):\")\n",
    "    print(f\"  Viability rate: {congress_df['viable'].mean():.1%}\")\n",
    "    print(f\"  Passage rate: {congress_df['passed'].mean():.1%}\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save models IN OPTIMIZED SPLIT COMPONENTS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODELS IN OPTIMIZED SPLIT COMPONENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'dataset_size': len(df),\n",
    "    'congresses': sorted(df['congress'].unique().tolist()),\n",
    "    'viable_rate': df['viable'].mean(),\n",
    "    'passage_rate': df['passed'].mean(),\n",
    "    'model_version': '6.0-6congress-optimized',\n",
    "    'improvements': [\n",
    "        'Trained on 6 congresses (113-118)',\n",
    "        'Uses days_active instead of days_since_introduction',\n",
    "        'Includes congress-specific features',\n",
    "        'Better temporal feature engineering',\n",
    "        'Optimized split components to avoid duplication',\n",
    "        'Combined small components into single file'\n",
    "    ],\n",
    "    'feature_sets': feature_sets\n",
    "}\n",
    "\n",
    "joblib.dump({\n",
    "    'metadata': metadata,\n",
    "    'label_encoders': {\n",
    "        'party': le_party,\n",
    "        'policy': le_policy\n",
    "    }\n",
    "}, '../models/metadata.pkl')\n",
    "print(\"✅ Saved metadata.pkl\")\n",
    "\n",
    "# Function to save model components optimally\n",
    "def save_model_components_optimized(models_dict, model_type):\n",
    "    \"\"\"Save each model stage as optimized components without duplication\"\"\"\n",
    "    for stage_name, model_data in models_dict.items():\n",
    "        # Create directory for this model\n",
    "        model_dir = f'../models/{model_type}_{stage_name}'\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n--- Saving {model_type}_{stage_name} components ---\")\n",
    "        \n",
    "        # Save RF model separately (it's the largest)\n",
    "        rf_file = f'{model_dir}/rf_model.pkl'\n",
    "        joblib.dump(model_data['rf_model'], rf_file)\n",
    "        rf_size = os.path.getsize(rf_file) / (1024 * 1024)\n",
    "        print(f\"  ✅ Saved rf_model.pkl ({rf_size:.1f} MB)\")\n",
    "        \n",
    "        # Combine smaller components into one file\n",
    "        small_components = {\n",
    "            'gb_model': model_data['gb_model'],\n",
    "            'lr_model': model_data['lr_model'],\n",
    "            'scaler': model_data['scaler'],\n",
    "            'selector': model_data['selector'],\n",
    "            'metadata': {\n",
    "                'features': model_data['features'],\n",
    "                'selected_features': model_data['selected_features'],\n",
    "                'threshold': model_data['threshold'],\n",
    "                'performance': model_data['performance'],\n",
    "                'is_calibrated': model_data['is_calibrated']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        components_file = f'{model_dir}/components.pkl'\n",
    "        joblib.dump(small_components, components_file)\n",
    "        components_size = os.path.getsize(components_file) / (1024 * 1024)\n",
    "        print(f\"  ✅ Saved components.pkl ({components_size:.1f} MB) - contains gb_model, lr_model, scaler, selector, metadata\")\n",
    "        \n",
    "        # Save ensemble configuration (not the models themselves)\n",
    "        ensemble_config = {\n",
    "            'voting': 'soft',\n",
    "            'weights': [0.4, 0.4, 0.2],\n",
    "            'estimator_names': ['rf', 'gb', 'lr']\n",
    "        }\n",
    "        \n",
    "        ensemble_config_file = f'{model_dir}/ensemble_config.pkl'\n",
    "        joblib.dump(ensemble_config, ensemble_config_file)\n",
    "        print(f\"  ✅ Saved ensemble_config.pkl (<0.1 MB)\")\n",
    "        \n",
    "        # If calibrated, save calibration data separately\n",
    "        if model_data['is_calibrated']:\n",
    "            # Extract calibration data from the CalibratedClassifierCV\n",
    "            calibrated_model = model_data['model']\n",
    "            calibration_data = {\n",
    "                'method': 'isotonic',\n",
    "                'cv': 3,\n",
    "                # Store the calibration mapping if accessible\n",
    "                'calibrators': []\n",
    "            }\n",
    "            \n",
    "            # Try to extract calibrators if they exist\n",
    "            if hasattr(calibrated_model, 'calibrated_classifiers_'):\n",
    "                for cal_clf in calibrated_model.calibrated_classifiers_:\n",
    "                    if hasattr(cal_clf, 'calibrator_'):\n",
    "                        calibration_data['calibrators'].append({\n",
    "                            'calibrator': cal_clf.calibrator_\n",
    "                        })\n",
    "            \n",
    "            calibration_file = f'{model_dir}/calibration.pkl'\n",
    "            joblib.dump(calibration_data, calibration_file)\n",
    "            cal_size = os.path.getsize(calibration_file) / (1024 * 1024)\n",
    "            print(f\"  ✅ Saved calibration.pkl ({cal_size:.1f} MB)\")\n",
    "        \n",
    "        # Calculate total size\n",
    "        total_size = rf_size + components_size + 0.1  # 0.1 for ensemble config\n",
    "        if model_data['is_calibrated']:\n",
    "            total_size += cal_size\n",
    "        \n",
    "        print(f\"  Total size for {model_type}_{stage_name}: {total_size:.1f} MB\")\n",
    "\n",
    "# Save viability models\n",
    "save_model_components_optimized(viability_models, 'viability')\n",
    "\n",
    "# Save passage models\n",
    "save_model_components_optimized(passage_models, 'passage')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING COMPLETE!\")\n",
    "print(f\"Models saved in 'models' directory as optimized components\")\n",
    "print(f\"Large RF models saved separately, small components combined\")\n",
    "print(f\"Trained on {len(df)} bills from congresses: {sorted(df['congress'].unique())}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7d404-ae48-4011-9893-8e7387a89429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
